{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from layer import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        channel = 512\n",
    "        G = []\n",
    "        G.append(ConvBlock(in_channels = 100, out_channels = channel, norm_layer = 'None', padding_mode = 'None',\n",
    "                           activation_fn = 'ReLU', conv = 'convT',  kernel_size = 3, stride = 1, padding = 0, bias = True))\n",
    "        for i in range(4):\n",
    "            G.append(ConvBlock(in_channels = channel, out_channels = channel//2, norm_layer = 'None', padding_mode = 'None',\n",
    "                               activation_fn = 'ReLU', conv = 'convT', kernel_size = 4, stride = 2, padding = 1, bias = True))\n",
    "            channel //= 2\n",
    "            \n",
    "        G.append(ConvBlock(in_channels = channel, out_channels = 3, norm_layer = 'None', padding_mode = 'None',\n",
    "                           activation_fn = 'Sigmoid', conv = 'convT', kernel_size = 4, stride = 2, padding = 1, bias = True))\n",
    "        \n",
    "        self.G = nn.Sequential(*G)\n",
    "#         self.G = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(in_channels = 100, out_channels = 512, kernel_size = 4, stride = 1, padding = 0, bias = False),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(True),\n",
    "            \n",
    "#             nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(True),\n",
    "            \n",
    "#             nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(True),\n",
    "            \n",
    "#             nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(True),\n",
    "            \n",
    "#             nn.ConvTranspose2d(in_channels = 64, out_channels = 3, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.G(inputs)\n",
    "    \n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        D = []\n",
    "        channel = 32\n",
    "        D.append(ConvBlock(in_channels = 3, out_channels = channel, norm_layer = 'None', padding_mode = 'None',\n",
    "                           activation_fn = 'LeakyReLU', conv = 'conv', kernel_size = 3, stride = 2, padding = 1, bias = True))\n",
    "        for i in range(4):\n",
    "            D.append(ConvBlock(in_channels = channel, out_channels = channel * 2, norm_layer = 'None', padding_mode = 'None',\n",
    "                               activation_fn = 'LeakyReLU', conv = 'conv', kernel_size = 4, stride = 2, padding = 1, bias = True))\n",
    "            channel *= 2\n",
    "        D.append(ConvBlock(in_channels = channel, out_channels = 1, norm_layer = 'None', padding_mode = 'None',\n",
    "                           activation_fn = 'None', conv = 'conv', kernel_size = 3, stride = 1, padding = 0, bias = True))\n",
    "        self.D = nn.Sequential(*D)\n",
    "        \n",
    "#         self.D = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "#             nn.LeakyReLU(0.2, inplace = True),\n",
    "            \n",
    "#             nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(0.2, inplace = True),\n",
    "            \n",
    "#             nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(0.2, inplace = True),\n",
    "            \n",
    "#             nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(0.2, inplace = True),\n",
    "            \n",
    "#             nn.Conv2d(in_channels = 512, out_channels = 1, kernel_size = 4, stride = 1, padding = 0, bias = False),\n",
    "#             nn.Sigmoid()\n",
    "            \n",
    "#         )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.D(inputs)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator()\n",
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 3, 3])\n",
      "torch.Size([1, 256, 6, 6])\n",
      "torch.Size([1, 128, 12, 12])\n",
      "torch.Size([1, 64, 24, 24])\n",
      "torch.Size([1, 32, 48, 48])\n",
      "torch.Size([1, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "r = torch.randn(1,100,1,1)\n",
    "for g in G.G:\n",
    "    r = g(r)\n",
    "    print(r.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 48, 48])\n",
      "torch.Size([1, 64, 24, 24])\n",
      "torch.Size([1, 128, 12, 12])\n",
      "torch.Size([1, 256, 6, 6])\n",
      "torch.Size([1, 512, 3, 3])\n",
      "torch.Size([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "r = torch.randn(1,3,96,96)\n",
    "for d in D.D:\n",
    "    r = d(r)\n",
    "    print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
